Directory structure:
└── job_scraper/
    ├── README.md
    ├── Project_Completion.txt
    ├── config.py
    ├── data.db
    ├── main.py
    ├── requirements.txt
    ├── resume.txt
    ├── database/
    │   ├── __init__.py
    │   ├── db_manager.py
    │   └── __pycache__/
    ├── docs/
    │   ├── Project Duties.docx
    │   └── Project Outline.docx
    ├── logs/
    ├── output/
    ├── scripts/
    │   ├── analyze_parsers.py
    │   ├── job_scrape.py
    │   ├── regex_extract.py
    │   ├── sbert_parser.py
    │   ├── tfidf_parser.py
    │   └── __pycache__/
    └── utils/
        ├── __init__.py
        ├── db_utils.py
        ├── logging.py
        └── __pycache__/

================================================
File: README.md
================================================
# Finance Job Scraping & Forecasting System

## Overview
This project is an automated system designed to scrape finance-related job postings, analyze and match job descriptions against a general-purpose finance resume, classify postings by sector, and forecast future job opportunities. It leverages multiple technologiesâ€”including API integrations, NLP techniques, SQL data storage, and (upcoming) interactive visualizationsâ€”to help users gain insights into the finance job market.

## Features
- **Automated Job Scraping:** Uses the JSearch API to pull finance job postings.
- **Data Storage:** Persists job data and analysis results in a SQLite database.
- **Resume Matching:** Computes similarity scores between job descriptions and a finance resume using both TF-IDF and SBERT approaches.
- **Analysis & Reporting:** Provides deep dive analysis with keyword insights and similarity metrics.
- **Planned Enhancements:** 
  - **Sector Classification:** Integration with YFinance API to categorize postings by finance sub-sector.
  - **Forecasting:** Development of ARIMA models to forecast job posting trends.
  - **Visualization:** Interactive dashboard built with Streamlit to explore current data and forecasts.

## Installation
1. **Clone the Repository:**
   git clone https://github.com/Team-Tu/JobScrape.git
   cd JobScrape
   
2. **Install Dependencies:**
   pip install -r requirements.txt
   
3. **Configuration:**
   - Update `config.py` with your API keys (for JSearch and YFinance) and adjust any file paths as necessary.

## Usage
- **Run the Pipeline:**
  Execute the main script to run the full pipelineâ€”from scraping to analysis:
  python main.py
  
- **Database:**
  The job postings and analysis results are stored in the SQLite database (`data.db`).

## Project Structure

job_scraper/
â”œâ”€â”€ config.py
â”œâ”€â”€ data.db
â”œâ”€â”€ main.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ resume.txt
â”œâ”€â”€ database/
â”œâ”€â”€ docs/
â”œâ”€â”€ logs/
â”œâ”€â”€ output/
â”œâ”€â”€ scripts/
â””â”€â”€ utils/

- **config.py:** Contains configuration settings and API keys.
- **main.py:** Orchestrates the entire pipeline.
- **scripts/**: Houses modules for job scraping, TF-IDF and SBERT parsing, and analysis.
- **database/**: Includes database management and schema setup scripts.
- **docs/**: Contains project documentation.
- **logs/** & **output/**: Used for log files and generated visualizations (e.g., charts).

## Future Enhancements
- **Sector Classification:** Integrate the YFinance API to fetch and store company sector information.
- **Forecasting:** Implement ARIMA-based models using historical data to predict future job trends.
- **Visualization:** Build an interactive Streamlit dashboard to display job trends, similarity scores, and forecasts.


================================================
File: Project_Completion.txt
================================================
Project Status and Next Steps

--------------------------------------------------
Completed:
--------------------------------------------------
1. **Job Scraping Pipeline:**
   - Implemented in `scripts/job_scrape.py` using the JSearch API.
   - Successfully retrieves finance-related job postings and stores them in the SQLite database.

2. **Database Schema:**
   - The SQLite database (`data.db`) is set up with the required tables (e.g., jobs, parser_results).
   - Database utilities and schema management are handled via scripts in the `database/` and `utils/` directories.

3. **Resume Parsing & Matching:**
   - TF-IDF based similarity scoring is implemented in `scripts/tfidf_parser.py`.
   - SBERT based similarity scoring is implemented in `scripts/sbert_parser.py`.
   - Analysis of each of the similarity scoring methods and deep dive functions (including saving parser results) are available in `scripts/analyze_parsers.py`.

4. **Basic Configuration & Logging:**
   - Configuration is managed in `config.py`.
   - A logging system is established via `utils/logging.py` to support debugging and performance tracking.

--------------------------------------------------
Next Steps / Pending Tasks:
--------------------------------------------------
1. **Sector Classification Integration:**
   - **Task:** Integrate the YFinance API to retrieve sector and industry data based on company tickers from job postings.
   - **How-To:** 
     - Use the YFinance API key (set in `config.py`) to fetch classification data.
     - Develop a new module or update an existing one to call the API and update the jobs database with sector details.
   - **Timeline:** Scheduled for Week 3 per project outline.

2. **Forecasting Future Job Postings:**
   - **Task:** Build an ARIMA-based time-series forecasting model to predict job posting trends.
   - **How-To:**
     - Extract historical job posting data from the database.
     - Utilize Pythonâ€™s `statsmodels` library to create and train an ARIMA model.
     - Integrate forecasting results into the system.
   - **Timeline:** Scheduled for Week 4.

3. **Interactive Visualization Dashboard:**
   - **Task:** Create a dashboard using Streamlit to visualize current job postings, similarity scores, historical trends, and forecasted job opportunities.
   - **How-To:**
     - Develop a Streamlit application that reads data from the SQLite database.
     - Include visual elements like histograms, bar charts, and trend graphs to provide an interactive user experience.
   - **Timeline:** Scheduled for Week 5.

4. **Final Testing & Optimization:**
   - **Task:** Perform comprehensive testing (unit, integration, and performance) on the entire pipeline.
   - **How-To:**
     - Run test cases and benchmark the system.
     - Identify and resolve any bugs or performance bottlenecks.
     - Refine the codebase for stability and scalability.
   - **Timeline:** Scheduled for Week 6.

--------------------------------------------------
Overall Summary:
--------------------------------------------------
The core functionalitiesâ€”job scraping, database management, and resume-job matchingâ€”are fully implemented. The next phases will focus on enhancing the system with sector classification, 
forecasting capabilities, and interactive visualizations, which will provide a complete end-to-end solution for analyzing the finance job market.












================================================
File: config.py
================================================
# config.py

import os

# Database configuration
DATABASE_FILE = 'data.db'
# Get the base directory where config.py is located.
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
SCHEMA_FILE = os.path.join(BASE_DIR, 'database', 'schema.sql')

# Logging configuration
LOG_DIR = os.path.join(BASE_DIR, 'logs')
LOG_FILE = 'app.log'
LOG_LEVEL = 'INFO'

# API keys and endpoints (placeholders, update with your actual keys)
JSEARCH_API = "16c7f72bd6mshf7c7c426eddf90bp173935jsn9286538f70b0"







================================================
File: data.db
================================================
[Non-text file]


================================================
File: main.py
================================================
#!/usr/bin/env python3
import os
from config import DATABASE_FILE
from scripts.job_scrape import main as scrape_jobs_main
from scripts.tfidf_parser import update_tfidf_similarity
from scripts.sbert_parser import update_sbert_similarity
from scripts.analyze_parsers import run_analysis
from scripts.regex_extract import main as regex_extract_main

def run_pipeline():
    # Step 1: Scrape job postings and save to database
    # Uncomment the following line to scrape jobs but only do this when necessary as all jobs are stored in the database that have been pulled previously
    # We only have 50,000 jobs for the api limit; do not run this!
    # scrape_jobs_main()

    # Step 2: Update similarity scores using resume.txt
    resume_path = os.path.join(os.getcwd(), "resume.txt")
    update_tfidf_similarity(resume_path, DATABASE_FILE)
    update_sbert_similarity(DATABASE_FILE, resume_path)

    # Step 3: Regex extraction and Final Similarity Score Calculation
    regex_extract_main()

    # Step 4: Run analysis/validation on the updated job data
    run_analysis()

if __name__ == "__main__":
    run_pipeline()



================================================
File: requirements.txt
================================================
[Non-text file]


================================================
File: resume.txt
================================================
RYAN M. LOVELESS
1421 James Way Erie, CO 80516  |  720-561-0071
LovelessRyanMitchell@Gmail.com  |  Github.com/RyLoveQuantStats  |  linkedin.com/in/ryanmloveless
________________________________________
PROFESSIONAL SUMMARY
Quantitative trader and algorithmic developer with expertise in futures, cryptocurrencies, and equities. Proven track record in building and optimizing medium-frequency trading strategies that generate consistent returns across various asset classes. Founder of RyLove Quantitative Statistics, mentoring traders and leveraging data-driven decision-making. Strong background in python development for finding alphas with machine learning, risk management, and advanced statistical analysis
________________________________________
EDUCATION
University of Colorado at Boulder â€“ Leeds School of Business 		       	          May 2025
Master of Science in Finance â€“ Financial Data Track					           GPA: 3.9           
Relevant Coursework: Quantitative Methods in R, Textual Analysis in Python, Applied Derivatives

Missouri State University, Springfield, MO						          Dec 2018
Bachelor of Science in Cell and Molecular Biology, Biochemistry Minor			           GPA: 3.5 
Relevant Coursework: Computational Calculus, Statistics in Biology, Physics I/II     
________________________________________
PROFESSIONAL EXPERIENCE
RyLove Quantitative Statistics, LLC						   March 2021 â€“ Present
Founder, Quantitative Researcher & Trader						 Sioux Falls, SD
â€¢	Designed, developed, and executed medium-frequency trading algorithms across futures, cryptocurrencies, equities, and commodities, leveraging Python, R, and Pine Script to generate consistent, alpha-driven returns.
â€¢	Integrated a comprehensive Python quant stack, including NumPy, SciPy, pandas, statsmodels, PyCaret, and TensorFlow, for numerical computing, data visualization, machine learning, and model optimization to enhance trading strategies and research outcomes.
â€¢	Developed and deployed systematic backtesting frameworks using tools like Zipline and Interactive Broker APIs to rigorously evaluate strategies, optimize Sharpe ratios, and ensure robust execution in high-volatility environments.
â€¢	Engineered machine learning-powered risk management systems in Python, utilizing libraries such as Pyfolio, Alphlens, and Riskfolio-Lib, to dynamically adjust position sizing and hedge portfolio risk in real-time.
â€¢	Mentored and led a team of traders, teaching advanced quantitative trading techniques and implementing automated workflows to improve execution efficiency and market engagement.
â€¢	Conducted in-depth quantitative market analysis in R, using statistical models and tools to identify profitable trading opportunities and validate hypotheses prior to live deployment.
â€¢	Integrated data visualization tools like Matplotlib and Seaborn for performance analysis and real-time strategy monitoring, enhancing transparency and decision-making.

Poseida Therapeutics 								         Sept â€™22 â€“ Aug â€˜23
Gene Therapy Research Associate II							  San Diego, CA

â€¢	Designed and executed quantitative models in Python and R to analyze gene therapy data sets, optimizing experimental designs and increasing assay efficiency.
â€¢	Developed automated Excel dashboards to analyze large-scale molecular biology datasets, improving data processing time by 30% for molecular biology ddPCR and qPCR assays.
â€¢	Applied statistical analysis in R to identify trends in qPCR/ddPCR and Western Blot data, ensuring experimental consistency and accuracy.
â€¢	Built models using Python and visualization in GraphPad Prism to analyze tissue pharmacokinetics, improving accuracy in mRNA and vector copy number (VCN) quantification.
â€¢	Utilized SQL and LIMS (Laboratory Information Management Systems) for data management and retrieval, ensuring streamlined workflows and traceability.

Inscripta Inc. 									        Dec. â€™21 â€“ Sept â€˜22     
Next Generation Sequencing Research Associate					                  Boulder, CO

â€¢	Developed R-based scripts to clean and analyze next-generation sequencing (NGS) data, improving error detection and batch consistency.
â€¢	Applied Python and Excel VBA automation to streamline data pipeline workflows, reducing manual processing time for sequencing results by 40%.
â€¢	Visualized large genomic datasets using Pythonâ€™s Matplotlib and Seaborn, identifying key trends in E. coli gene knockdown experiments.
â€¢	Implemented statistical quality control methods in R to optimize nucleic acid extraction and library preparation protocols.
â€¢	Collaborated with the NGS Core Team to troubleshoot and improve data analysis workflows, ensuring accuracy in high-throughput sequencing experiments.
________________________________________
SKILLS
â€¢	Software Proficiency: Python, R, SQL, Pine Script 
â€¢	Trading Platforms: Bloomberg, FactSet, NinjaTrader, MetaTrader, Fidelity Active Trader Pro, TradingView, Interactive Brokers API
â€¢	Technicals: 
â€¢	High & Medium-Frequency Algorithmic Trading in Python 
â€¢	Statistical Analysis in R, Data Management in SQL 
â€¢	Machine Learning in Python (inspired by Advances in Financial Machine Learning â€“ LÃ³pez de Prado) 
â€¢	Certifications & Self-Study: 
â€¢	Bloomberg Market Concepts; Microsoft Excel Expert
â€¢	Python for Financial Analysis & Algorithmic Trading (Udemy), Python for Data Science and Machine Learning (Udemy), SQL for Data Science (Coursera)
â€¢	Applied concepts from Python for Algorithmic Trading Cookbook (E. Chan), Finding Alpha (E. Dubofsky), Options Volatility & Pricing (S. Natenberg) in developing systematic trading strategies.
â€¢	Interests: Finding alpha, outdoor rock climbing, fly fishing, studying for CFA Level 1
â€¢	Achievements: Ranked in the top 5% globally in the 2024 Bloomberg Equity Trading Challenge



================================================
File: database/__init__.py
================================================



================================================
File: database/db_manager.py
================================================
# db_manager.py

import sqlite3
from utils.db_utils import create_connection, execute_query, execute_read_query

class DBManager:
    def __init__(self, db_file):
        self.db_file = db_file
        self.conn = create_connection(db_file)
    
    def add_job(self, title, company, location, description, ticker, posted_date, similarity_score=None):
        """
        Insert a new job posting into the jobs table.
        """
        query = """
        INSERT INTO jobs (title, company, location, description, ticker, posted_date, similarity_score)
        VALUES (?, ?, ?, ?, ?, ?, ?)
        """
        params = (title, company, location, description, ticker, posted_date, similarity_score)
        return execute_query(self.conn, query, params)
    
    def get_jobs(self, filters=None):
        """
        Retrieve job postings. Optionally accepts a filters dictionary, e.g.,
        {'ticker': 'AAPL', 'location': 'New York'}.
        """
        query = "SELECT * FROM jobs"
        if filters:
            conditions = []
            params = []
            for key, value in filters.items():
                conditions.append(f"{key} = ?")
                params.append(value)
            query += " WHERE " + " AND ".join(conditions)
            return execute_read_query(self.conn, query, params)
        else:
            return execute_read_query(self.conn, query)
    
    def update_similarity(self, job_id, similarity_score):
        """
        Update the similarity score for a given job posting.
        """
        query = "UPDATE jobs SET similarity_score = ? WHERE job_id = ?"
        params = (similarity_score, job_id)
        return execute_query(self.conn, query, params)
    
    def add_or_update_company(self, name, ticker, sector, industry):
        """
        Add a new company or update an existing companyâ€™s details.
        """
        select_query = "SELECT company_id FROM companies WHERE ticker = ?"
        result = execute_read_query(self.conn, select_query, (ticker,))
        if result and len(result) > 0:
            # Update existing record
            query = "UPDATE companies SET name = ?, sector = ?, industry = ? WHERE ticker = ?"
            params = (name, sector, industry, ticker)
        else:
            # Insert new record
            query = "INSERT INTO companies (name, ticker, sector, industry) VALUES (?, ?, ?, ?)"
            params = (name, ticker, sector, industry)
        return execute_query(self.conn, query, params)
    
    def get_company(self, ticker):
        """
        Retrieve company details by ticker.
        """
        query = "SELECT * FROM companies WHERE ticker = ?"
        return execute_read_query(self.conn, query, (ticker,))
    
    def close(self):
        """
        Close the database connection.
        """
        if self.conn:
            self.conn.close()




================================================
File: docs/Project Duties.docx
================================================
[Non-text file]


================================================
File: docs/Project Outline.docx
================================================
[Non-text file]




================================================
File: scripts/analyze_parsers.py
================================================
#!/usr/bin/env python3
import os
import re
import sqlite3
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import logging
from datetime import datetime
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer
import nltk
nltk.download('punkt', quiet=True)
from nltk.tokenize import word_tokenize

from config import DATABASE_FILE
from utils.logging import setup_logging

logger = setup_logging()

# ---------------------
# TABLE CREATION FOR PARSER RESULTS
# ---------------------
PARSER_RESULTS_SQL = """
CREATE TABLE IF NOT EXISTS parser_results (
    result_id INTEGER PRIMARY KEY AUTOINCREMENT,
    job_id INTEGER,
    parser_type TEXT,         -- 'TF-IDF' or 'SBERT'
    keyword TEXT,
    ranking INTEGER,          -- 1 to 10
    score REAL,
    overall_similarity REAL,  -- overall similarity score for the job
    analysis_timestamp DATETIME DEFAULT CURRENT_TIMESTAMP
);
"""

def create_parser_results_table(db_file):
    """Create the parser_results table if it does not exist."""
    try:
        conn = sqlite3.connect(db_file)
        cur = conn.cursor()
        cur.execute(PARSER_RESULTS_SQL)
        conn.commit()
        logger.info("Table 'parser_results' created or already exists.")
    except sqlite3.Error as e:
        logger.error("Error creating parser_results table: %s", e)
    finally:
        conn.close()

def save_parser_results(db_file, job_id, parser_type, overall_similarity, results):
    """
    Save parser analysis results into the parser_results table.
    'results' is a list of tuples (keyword, score), sorted by ranking.
    Convert the score to float to ensure it's stored as a numeric value.
    """
    try:
        conn = sqlite3.connect(db_file)
        cur = conn.cursor()
        for ranking, (keyword, score) in enumerate(results, start=1):
            float_score = float(score)  # Make sure we store a numeric float
            query = """
                INSERT INTO parser_results (job_id, parser_type, keyword, ranking, score, overall_similarity)
                VALUES (?, ?, ?, ?, ?, ?)
            """
            cur.execute(query, (job_id, parser_type, keyword, ranking, float_score, overall_similarity))
        conn.commit()
        logger.info("Saved %d %s parser results for job_id %s.", len(results), parser_type, job_id)
    except sqlite3.Error as e:
        logger.error("Error saving parser results: %s", e)
    finally:
        conn.close()

# ---------------------
# Data Loading and Preprocessing
# ---------------------
def load_job_data(db_file):
    """Load job data (including similarity scores) into a DataFrame."""
    conn = sqlite3.connect(db_file)
    try:
        query = """
            SELECT job_id, title, company, description, similarity_score, similarity_score_sbert
            FROM jobs
        """
        df = pd.read_sql_query(query, conn)
        return df
    except Exception as e:
        logger.error("Error loading job data: %s", e)
        return pd.DataFrame()
    finally:
        conn.close()

def preprocess(text):
    """Simple text preprocessing."""
    text = text.lower()
    return re.sub(r'[\W_]+', ' ', text)

# ---------------------
# TF-IDF Analysis
# ---------------------
def compute_tfidf_details(resume_text, job_text):
    """
    Compute TFâ€‘IDF vectors for two texts and return the cosine similarity
    and top 10 unique keyword contributions.
    """
    docs = [preprocess(resume_text), preprocess(job_text)]
    vectorizer = TfidfVectorizer(stop_words='english')
    tfidf = vectorizer.fit_transform(docs)
    resume_vec = tfidf[0].toarray().flatten()
    job_vec = tfidf[1].toarray().flatten()
    similarity = cosine_similarity(tfidf[0], tfidf[1])[0, 0]
    contributions = resume_vec * job_vec
    features = vectorizer.get_feature_names_out()
    sorted_idx = np.argsort(contributions)[::-1]
    # Collect unique keywords (they will be unique since features are unique)
    top_keywords = [(features[i], contributions[i]) for i in sorted_idx if contributions[i] > 0][:10]
    return similarity, top_keywords

# ---------------------
# SBERT Analysis (Unique Tokens)
# ---------------------
def analyze_sbert_contributions(resume_text, job_text, model_name="all-MiniLM-L6-v2"):
    """
    Approximate SBERT interpretability by tokenizing the resume,
    encoding each token with SBERT, and computing the cosine similarity
    between each token and the job description embedding.
    Returns the top 10 unique tokens (lowercase) with highest similarity scores.
    """
    tokens = word_tokenize(resume_text)
    tokens = [t.lower() for t in tokens if len(t) > 1]
    model = SentenceTransformer(model_name)
    job_embedding = model.encode([job_text])[0]
    token_embeddings = model.encode(tokens)
    similarities = cosine_similarity(token_embeddings, [job_embedding]).flatten()
    unique_tokens = {}
    for token, sim in zip(tokens, similarities):
        # Keep highest similarity for each unique token
        if token in unique_tokens:
            if sim > unique_tokens[token]:
                unique_tokens[token] = sim
        else:
            unique_tokens[token] = sim
    sorted_tokens = sorted(unique_tokens.items(), key=lambda x: x[1], reverse=True)
    return sorted_tokens[:10]

# ---------------------
# Visualization Functions
# ---------------------
def plot_histograms(df):
    """Generate histograms for the TF-IDF and SBERT similarity scores."""
    print("Basic Statistics:")
    print(df[['similarity_score', 'similarity_score_sbert']].describe())

    plt.figure(figsize=(12,5))
    plt.subplot(1,2,1)
    sns.histplot(df['similarity_score'].dropna(), kde=True, color='blue')
    plt.title("TFâ€‘IDF Score Distribution")
    plt.xlabel("TFâ€‘IDF Score")

    plt.subplot(1,2,2)
    sns.histplot(df['similarity_score_sbert'].dropna(), kde=True, color='green')
    plt.title("SBERT Score Distribution")
    plt.xlabel("SBERT Score")
    plt.tight_layout()
    plt.savefig("output/similarity_histograms.png")
    plt.show()

# ---------------------
# Baseline Calculation
# ---------------------
def compute_baselines(df):
    """Compute recommended baseline thresholds (75th percentile) for both models."""
    tfidf_base = np.percentile(df['similarity_score'].dropna(), 75)
    sbert_base = np.percentile(df['similarity_score_sbert'].dropna(), 75)
    print("\nRecommended Baseline Thresholds:")
    print(f" - TFâ€‘IDF: {tfidf_base:.4f}")
    print(f" - SBERT:  {sbert_base:.4f}")
    return tfidf_base, sbert_base

# ---------------------
# Bar Chart of Top 10 Words from Each Model
# ---------------------
def plot_top10_keywords(tfidf_keywords, sbert_tokens):
    """
    Show all top 10 words from TF-IDF and top 10 tokens from SBERT in a single bar chart.
    We'll do an outer merge on the 'word' to compare their scores side by side.
    """
    df_tfidf = pd.DataFrame(tfidf_keywords, columns=['word', 'tfidf_score'])
    df_sbert = pd.DataFrame(sbert_tokens, columns=['word', 'sbert_score'])

    # Merge outer on 'word'
    df_merge = pd.merge(df_tfidf, df_sbert, on='word', how='outer')
    df_merge.set_index('word', inplace=True)

    # Sort by highest TF-IDF or SBERT (just for a consistent display)
    df_merge.sort_values(by=['tfidf_score','sbert_score'], ascending=False, inplace=True, na_position='last')

    # Plot as bar chart
    df_merge.plot(kind='bar', figsize=(10,6))
    plt.title("Top 10 TFâ€‘IDF vs SBERT Keywords")
    plt.ylabel("Score")
    plt.tight_layout()
    plt.savefig("output/top10_keywords_barchart.png")
    plt.show()

# ---------------------
# Deep Dive Analysis and Saving Results
# ---------------------
def deep_dive(df, resume_path):
    """Perform deep dive analysis on the top matching job for each model."""
    with open(resume_path, 'r', encoding='utf-8') as f:
        resume_text = f.read()
    
    # Deep dive for TF-IDF
    top_tfidf = df.sort_values("similarity_score", ascending=False).iloc[0]
    print("\n--- Deep Dive: Top TFâ€‘IDF Matching Job ---")
    print(f"Job ID: {top_tfidf['job_id']}, Title: {top_tfidf['title']}, Company: {top_tfidf['company']}")
    print("Description (first 500 chars):")
    print(top_tfidf['description'][:500] + "...")
    tfidf_sim, tfidf_keywords = compute_tfidf_details(resume_text, top_tfidf['description'])
    print(f"Recomputed TFâ€‘IDF Cosine Similarity: {tfidf_sim:.4f}")
    print("Top contributing keywords (TFâ€‘IDF):")
    for word, contrib in tfidf_keywords:
        print(f"  {word}: {contrib:.4f}")
    
    # Save TF-IDF results into database.
    save_parser_results(DATABASE_FILE, top_tfidf['job_id'], "TF-IDF", tfidf_sim, tfidf_keywords)
    
    # Deep dive for SBERT
    top_sbert = df.sort_values("similarity_score_sbert", ascending=False).iloc[0]
    print("\n--- Deep Dive: Top SBERT Matching Job ---")
    print(f"Job ID: {top_sbert['job_id']}, Title: {top_sbert['title']}, Company: {top_sbert['company']}")
    print("Description (first 500 chars):")
    print(top_sbert['description'][:500] + "...")
    sbert_tokens = analyze_sbert_contributions(resume_text, top_sbert['description'])
    print("Top contributing tokens from resume (SBERT, unique):")
    for token, score in sbert_tokens:
        print(f"  {token}: {score:.4f}")
    
    # Save SBERT results into database.
    overall_sbert = top_sbert['similarity_score_sbert']
    save_parser_results(DATABASE_FILE, top_sbert['job_id'], "SBERT", overall_sbert, sbert_tokens)

    # Plot bar chart with all top 10 keywords from both methods
    plot_top10_keywords(tfidf_keywords, sbert_tokens)

def deep_dive_and_save(df, resume_path):
    """Wrapper for deep dive analysis that also saves parser results."""
    deep_dive(df, resume_path)

# ---------------------
# Validation Function
# ---------------------
def run_validation():
    """Run full validation, deep dive analysis, and baseline calculation."""
    df = load_job_data(DATABASE_FILE)
    if df.empty:
        logger.error("No job data found for validation.")
        return
    resume_path = os.path.join(os.getcwd(), "resume.txt")

    # Plot only histograms (scatter plot removed)
    plot_histograms(df)

    # Perform deep dive and save results
    deep_dive_and_save(df, resume_path)

    # Compute baseline thresholds
    compute_baselines(df)

    logger.info("Validation complete.")

def run_analysis():
    create_parser_results_table(DATABASE_FILE)
    run_validation()

if __name__ == "__main__":
    run_analysis()



================================================
File: scripts/job_scrape.py
================================================
#!/usr/bin/env python3
import os
import time
import requests
import sqlite3
import logging

from config import JSEARCH_API, DATABASE_FILE
from utils.logging import setup_logging

logger = setup_logging()

# -------------------
# JSearch API Configuration
# -------------------
API_HOST = "jsearch.p.rapidapi.com"
BASE_URL = "https://jsearch.p.rapidapi.com"
HEADERS = {
    "x-rapidapi-key": JSEARCH_API,
    "x-rapidapi-host": API_HOST
}

# -------------------
# Retry Configuration
# -------------------
MAX_RETRIES = 3
RETRY_DELAY = 5  # seconds

# -------------------
# Database Schema: Jobs Table (extended)
# -------------------
JOBS_TABLE_SQL = """
CREATE TABLE IF NOT EXISTS jobs (
    job_id INTEGER PRIMARY KEY AUTOINCREMENT,
    jsearch_job_id TEXT UNIQUE,   -- Unique job ID from JSearch
    title TEXT NOT NULL,
    company TEXT NOT NULL,
    location TEXT,
    description TEXT,
    ticker TEXT,
    posted_date TEXT,
    scraped_date TEXT DEFAULT (datetime('now')),
    similarity_score REAL,
    similarity_score_sbert REAL,
    details TEXT,               -- JSON from /job-details
    estimated_salary TEXT,      -- JSON from /estimated-salary
    company_salary TEXT         -- JSON from /company-job-salary
);
"""

def initialize_jobs_table(db_file):
    """
    Create the jobs table if it does not exist.
    """
    try:
        conn = sqlite3.connect(db_file)
        cur = conn.cursor()
        cur.execute(JOBS_TABLE_SQL)
        conn.commit()
        logger.info("Jobs table created or already exists.")
    except sqlite3.Error as e:
        logger.error("Error creating jobs table: %s", e)
    finally:
        if conn:
            conn.close()

# -------------------
# General API Call Function
# -------------------
def call_api(endpoint, params):
    """
    Makes an API call with retry logic.
    """
    for attempt in range(MAX_RETRIES):
        try:
            response = requests.get(BASE_URL + endpoint, headers=HEADERS, params=params)
            if response.status_code == 429:
                logger.warning("Rate limit hit on %s. Waiting for %s seconds...", endpoint, RETRY_DELAY)
                time.sleep(RETRY_DELAY)
                continue
            elif response.status_code != 200:
                logger.error("Error: Received status code %s for endpoint %s with params: %s", 
                             response.status_code, endpoint, params)
                return None
            return response
        except Exception as e:
            logger.exception("Exception during API call to %s: %s", endpoint, e)
            time.sleep(RETRY_DELAY)
    return None

# -------------------
# Endpoint-Specific Functions
# -------------------
def search_jobs(query, location="us", page=1, num_pages=10):
    """
    Calls the /search endpoint to search for jobs.
    Returns a list of job objects.
    num_pages=10 returns roughly 100 jobs (10 per page).
    """
    endpoint = "/search"
    params = {
        "query": query,
        "location": location,
        "page": page,
        "num_pages": num_pages
    }
    response = call_api(endpoint, params)
    if response:
        try:
            json_response = response.json()
            if json_response.get("status") != "OK":
                logger.error("API error in /search for query '%s': %s", query, json_response.get("error"))
                return []
            return json_response.get("data", [])
        except Exception as e:
            logger.exception("Error parsing JSON from /search for query '%s': %s", query, e)
    return []

def get_job_details(job_id):
    """
    Calls the /job-details endpoint for a given job_id.
    Returns a dictionary with job details.
    """
    endpoint = "/job-details"
    params = {"job_id": job_id}
    response = call_api(endpoint, params)
    if response:
        try:
            json_response = response.json()
            if json_response.get("status") != "OK":
                logger.error("API error in /job-details: %s", json_response.get("error"))
                return {}
            return json_response.get("data", {})
        except Exception as e:
            logger.exception("Error parsing JSON from /job-details: %s", e)
    return {}

def get_estimated_salary(job_title, location="us"):
    """
    Calls the /estimated-salary endpoint for a given job title and location.
    Returns a dictionary with salary info.
    """
    endpoint = "/estimated-salary"
    params = {
        "job_title": job_title,
        "location": location
    }
    response = call_api(endpoint, params)
    if response:
        try:
            json_response = response.json()
            if json_response.get("status") != "OK":
                logger.error("API error in /estimated-salary: %s", json_response.get("error"))
                return {}
            return json_response.get("data", {})
        except Exception as e:
            logger.exception("Error parsing JSON from /estimated-salary: %s", e)
    return {}

def get_company_job_salary(job_title, company, location="us"):
    """
    Calls the /company-job-salary endpoint for a given job title and company (and optional location).
    Returns a dictionary with company salary info.
    """
    endpoint = "/company-job-salary"
    params = {
        "job_title": job_title,
        "company": company,
        "location": location
    }
    response = call_api(endpoint, params)
    if response:
        try:
            json_response = response.json()
            if json_response.get("status") != "OK":
                logger.error("API error in /company-job-salary: %s", json_response.get("error"))
                return {}
            return json_response.get("data", {})
        except Exception as e:
            logger.exception("Error parsing JSON from /company-job-salary: %s", e)
    return {}

# -------------------
# Database Insertion / Update Helpers
# -------------------
def insert_job(conn, job):
    """
    Insert a job (from /search endpoint) into the jobs table.
    """
    try:
        cur = conn.cursor()
        title = job.get("job_title", "N/A")
        company = job.get("employer_name", "N/A")
        city = job.get("job_city", "")
        country = job.get("job_country", "")
        location_str = f"{city}, {country}" if city or country else "Location not provided"
        description = job.get("job_description", "N/A")
        ticker = job.get("ticker", None)
        posted_date = job.get("posted_date", None)
        jsearch_job_id = job.get("job_id")
        cur.execute("""
            INSERT OR IGNORE INTO jobs (jsearch_job_id, title, company, location, description, ticker, posted_date)
            VALUES (?, ?, ?, ?, ?, ?, ?)
        """, (jsearch_job_id, title, company, location_str, description, ticker, posted_date))
        conn.commit()
    except sqlite3.Error as e:
        logger.error("Error inserting job: %s", e)

def update_job_with_details(conn, jsearch_job_id, details, estimated_salary, company_salary):
    """
    Update the job record with additional data from /job-details, /estimated-salary, and /company-job-salary endpoints.
    """
    try:
        cur = conn.cursor()
        cur.execute("""
            UPDATE jobs 
            SET details = ?, estimated_salary = ?, company_salary = ?
            WHERE jsearch_job_id = ?
        """, (str(details), str(estimated_salary), str(company_salary), jsearch_job_id))
        conn.commit()
    except sqlite3.Error as e:
        logger.error("Error updating job with details: %s", e)

# -------------------
# Main Function
# -------------------
def main():
    logger.info("Starting job scraping using multiple JSearch queries...")

    # Initialize the jobs table.
    initialize_jobs_table(DATABASE_FILE)
    
    # Define a list of search queries for broader finance jobs.
    search_queries = [
        "quantitative trading"

        '''
        "quantitative researcher", "quantitative developer", "trading analyst",
        "trading operations", "trading associate", "finance",
        "portfolio management", "investment banking", "financial analyst", "CPA",
        "financial advisor", "financial consultant", "financial planner",
        "financial analyst", "risk management", "financial modeling",
        "financial reporting", "financial controller", "financial operations",
        "financial services", "investment analyst", "investment associate",
        "investment consultant", "investment manager", "investment advisor",
        "investment banking analyst", "investment banking associate", "CFA", "CQF"
        '''
        # Add more queries as needed...
    ]
    
    # Open a database connection.
    conn = sqlite3.connect(DATABASE_FILE)
    
    # Loop through each query, call the API, and insert the jobs.
    for query in search_queries:
        logger.info("Searching jobs with query: '%s'", query)
        jobs = search_jobs(query, num_pages=10)  # 10 pages * ~10 jobs per page = ~100 jobs per query.
        logger.info("Found %s jobs for query '%s'.", len(jobs), query)
        for job in jobs:
            insert_job(conn, job)
        # Optional: add a delay between queries to avoid rate limiting.
        time.sleep(2)
    
    # Now update each job record with additional details.
    cur = conn.cursor()
    cur.execute("SELECT jsearch_job_id, job_description, job_title, employer_name FROM jobs")
    jobs_info = cur.fetchall()
    logger.info("Updating details for %d jobs.", len(jobs_info))
    for jsearch_job_id, description, job_title, employer_name in jobs_info:
        # Only call salary endpoints if we have a non-empty job_title.
        if job_title and job_title.strip():
            details_data = get_job_details(jsearch_job_id)
            estimated_salary_data = get_estimated_salary(job_title)
            company_salary_data = get_company_job_salary(job_title, employer_name if employer_name else "")
        else:
            details_data = get_job_details(jsearch_job_id)
            estimated_salary_data = {}
            company_salary_data = {}
        update_job_with_details(conn, jsearch_job_id, details_data, estimated_salary_data, company_salary_data)

        
        conn.close()
    logger.info("Job scraping and endpoint extraction complete.")

if __name__ == "__main__":
    main()



================================================
File: scripts/regex_extract.py
================================================
#!/usr/bin/env python3
import re
import sqlite3
import logging
from config import DATABASE_FILE
from utils.logging import setup_logging

logger = setup_logging()

# Bonus weights
SALARY_BONUS = 0.1
COLORADO_BONUS = 0.15
EDUCATION_BONUS = 0.1

### Regex patterns:

SALARY_PATTERN = r"(?i)(?:salary|compensation|pay)[^\$]{0,20}\$\s*(\d{1,3}(?:,\d{3})+|\d{1,3})(?:\s*[kK])?" # Searches for a salary component in the job description
LOCATION_PATTERN = r"(?i)\b(?:Colorado|Colo\.|CO|Co\.|Denver|Den\.?|Boulder|Bld\.?)\b" # Searches for if Colorado, Denver or Boulder is mentioned in the job description
EDUCATION_PATTERN = r"\b(?:Master(?:'s)?\s+Degree|Master(?:'s)?\s+of\s+(?:Science\s+in\s+Finance|Finance)|MSF|MSc(?:\s+Finance)?)\b" # Searches for if a Master of Science in Finance is mentioned in the job description

def extract_salary(text):
    match = re.search(SALARY_PATTERN, text)
    if match:
        try:
            # Convert the captured salary figure to an integer.
            salary = int(match.group(1).replace(",", ""))
            return salary
        except ValueError:
            return None
    return None

def check_colorado(text):
    return bool(re.search(LOCATION_PATTERN, text, flags=re.IGNORECASE))

def check_education(text):
    return bool(re.search(EDUCATION_PATTERN, text, flags=re.IGNORECASE))

def load_jobs(db_file):
    conn = sqlite3.connect(db_file)
    cur = conn.cursor()
    # Fetch job_id, title, description, precomputed semantic similarity score, and location
    cur.execute("SELECT job_id, title, description, similarity_score_sbert, location FROM jobs")
    jobs = cur.fetchall()
    conn.close()
    return jobs

def add_final_score_column(db_file):
    """
    Adds a new column 'final_similarity_score' to the jobs table if it does not exist.
    SQLite doesn't support IF NOT EXISTS for ALTER TABLE, so we catch the error if the column exists.
    """
    conn = sqlite3.connect(db_file)
    cur = conn.cursor()
    try:
        cur.execute("ALTER TABLE jobs ADD COLUMN final_similarity_score REAL")
        conn.commit()
        logger.info("Column 'final_similarity_score' added to jobs table.")
    except sqlite3.OperationalError as e:
        if "duplicate column name" in str(e).lower():
            logger.info("Column 'final_similarity_score' already exists.")
        else:
            logger.error("Error adding column 'final_similarity_score': %s", e)
    finally:
        conn.close()

def compute_final_score(job):
    """
    For a given job tuple, compute the final ranking score based on:
    - The base semantic similarity score.
    - A bonus for salary if above 80K.
    - A bonus if the job is in Colorado.
    - A bonus if the education requirement includes 'Master of Science in Finance'.
    """
    job_id, title, description, base_score, db_location = job
    if base_score is None:
        base_score = 0.0

    text = f"{title or ''} {description or ''} {db_location or ''}"
    bonus = 0.0
    salary = extract_salary(text)
    if salary and salary >= 80000:
        bonus += SALARY_BONUS
        logger.debug("Job %s: Salary bonus added (salary: %s)", job_id, salary)
    if check_colorado(text):
        bonus += COLORADO_BONUS
        logger.debug("Job %s: Colorado bonus added", job_id)
    if check_education(text):
        bonus += EDUCATION_BONUS
        logger.debug("Job %s: Education bonus added", job_id)
    final_score = base_score + bonus
    return job_id, final_score

def update_final_scores_in_db(db_file, scored_jobs):
    """
    Update the final_similarity_score column in the jobs table for each job.
    """
    conn = sqlite3.connect(db_file)
    cur = conn.cursor()
    for job_id, final_score in scored_jobs:
        cur.execute("UPDATE jobs SET final_similarity_score = ? WHERE job_id = ?", (final_score, job_id))
    conn.commit()
    conn.close()
    logger.info("Final similarity scores updated in the database.")

def count_regex_matches(text):
    """
    Count and return the number of matches for salary, Colorado, and education patterns in the provided text.
    """
    salary_matches = re.findall(SALARY_PATTERN, text, flags=re.IGNORECASE)
    colorado_matches = re.findall(LOCATION_PATTERN, text, flags=re.IGNORECASE)
    education_matches = re.findall(EDUCATION_PATTERN, text, flags=re.IGNORECASE)
    return len(salary_matches), len(colorado_matches), len(education_matches)

def query_and_analyze(db_file):
    """
    Query the database for jobs and log how many times each regex pattern matched per job,
    along with aggregate counts.
    """
    conn = sqlite3.connect(db_file)
    cur = conn.cursor()
    cur.execute("SELECT job_id, title, description, location, final_similarity_score FROM jobs")
    jobs = cur.fetchall()
    conn.close()

    total_salary = 0
    total_colorado = 0
    total_education = 0
    job_count = 0

    for job in jobs:
        job_id, title, description, location, final_score = job
        text = f"{title or ''} {description or ''} {location or ''}"
        salary_count, colorado_count, education_count = count_regex_matches(text)
        total_salary += salary_count
        total_colorado += colorado_count
        total_education += education_count
        job_count += 1
        logger.info("Job ID: %s | Salary Matches: %d | Colorado Matches: %d | Education Matches: %d | Final Score: %s",
                    job_id, salary_count, colorado_count, education_count, final_score)
    
    logger.info("Processed %d jobs.", job_count)
    logger.info("Aggregate counts: Salary Matches: %d, Colorado Matches: %d, Education Matches: %d",
                total_salary, total_colorado, total_education)

def main():
    logger.info("Starting combined ranking and regex analysis pipeline...")
    
    # Ensure the final_similarity_score column exists.
    add_final_score_column(DATABASE_FILE)
    
    # Load jobs from the database.
    jobs = load_jobs(DATABASE_FILE)
    if not jobs:
        logger.error("No jobs found in the database.")
        return
    
    # Compute final scores for each job.
    scored_jobs = []
    for job in jobs:
        job_id, final_score = compute_final_score(job)
        scored_jobs.append((job_id, final_score))
    
    # Update the database with the final scores.
    update_final_scores_in_db(DATABASE_FILE, scored_jobs)
    
    # Sort jobs by final score (highest first) and log top 10 results.
    scored_jobs.sort(key=lambda x: x[1], reverse=True)
    logger.info("Top 10 ranked jobs based on final similarity score:")
    for job_id, score in scored_jobs[:10]:
        logger.info("Job ID: %s, Final Score: %.4f", job_id, score)
    
    # Query the database and perform regex analysis.
    query_and_analyze(DATABASE_FILE)

if __name__ == "__main__":
    main()


================================================
File: scripts/sbert_parser.py
================================================
#!/usr/bin/env python3
import os
import sqlite3
import logging
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

from config import DATABASE_FILE  # Adjust the path if necessary
from utils.logging import setup_logging

logger = setup_logging()

def load_resume_text(resume_path):
    """
    Load the resume text from a file.
    """
    try:
        with open(resume_path, "r", encoding="utf-8") as f:
            return f.read()
    except Exception as e:
        logger.exception("Error loading resume text: %s", e)
        raise

def compute_sbert_similarity(resume_text, job_texts, model_name="all-MiniLM-L6-v2"):
    """
    Compute cosine similarity scores between the resume and job texts using SBERT.
    Returns a 1D numpy array of similarity scores.
    """
    model = SentenceTransformer(model_name)
    # Encode returns numpy arrays
    resume_embedding = model.encode([resume_text])   # shape: (1, embedding_dim)
    job_embeddings = model.encode(job_texts)         # shape: (num_jobs, embedding_dim)

    # Compute cosine similarities (resume vs. each job)
    similarities = cosine_similarity(resume_embedding, job_embeddings)
    # similarities is shape (1, num_jobs). Flatten to shape (num_jobs,)
    return similarities.flatten()

def add_sbert_column_if_needed(conn):
    """
    Add a new column 'similarity_score_sbert' to the jobs table if it does not exist.
    SQLite doesn't support IF NOT EXISTS for ALTER TABLE, so we try and catch the error.
    """
    try:
        conn.execute("ALTER TABLE jobs ADD COLUMN similarity_score_sbert REAL")
        conn.commit()
        logger.info("Added column similarity_score_sbert to jobs table.")
    except sqlite3.OperationalError as e:
        if "duplicate column name" in str(e).lower():
            logger.info("Column similarity_score_sbert already exists.")
        else:
            logger.error("Error adding new column: %s", e)
            raise

def update_sbert_similarity(db_file, resume_path):
    """
    Retrieves all job descriptions from the jobs table, computes similarity scores
    with SBERT, and updates each record with the SBERT-based similarity score.
    """
    try:
        conn = sqlite3.connect(db_file)
        cur = conn.cursor()

        # Ensure the new column exists.
        add_sbert_column_if_needed(conn)

        # Retrieve job_id and description from the jobs table.
        cur.execute("SELECT job_id, description FROM jobs")
        jobs = cur.fetchall()
        if not jobs:
            logger.info("No jobs found in database to update.")
            return

        job_ids = [job_id for job_id, _ in jobs]
        job_descriptions = [desc for _, desc in jobs]

        # Load and preprocess the resume text.
        resume_text = load_resume_text(resume_path)

        # Compute SBERT similarity scores.
        scores = compute_sbert_similarity(resume_text, job_descriptions)
        logger.info("Computed SBERT similarity scores for %d jobs.", len(scores))

        # Update each job record with the SBERT similarity score.
        for job_id, score in zip(job_ids, scores):
            float_score = float(score)  # Ensure it's a plain Python float
            logger.debug("Updating job_id=%s with SBERT similarity=%f (type=%s)",
                         job_id, float_score, type(float_score))
            cur.execute("UPDATE jobs SET similarity_score_sbert = ? WHERE job_id = ?",
                        (float_score, job_id))
        conn.commit()
        logger.info("Updated SBERT similarity scores for %d jobs.", len(job_ids))
    except Exception as e:
        logger.exception("Error updating SBERT similarity scores: %s", e)
    finally:
        if conn:
            conn.close()

if __name__ == "__main__":
    # Adjust the path to your resume if necessary.
    resume_path = os.path.join(os.getcwd(), "resume.txt")
    update_sbert_similarity(DATABASE_FILE, resume_path)



================================================
File: scripts/tfidf_parser.py
================================================
import os
import re
import sqlite3
import logging
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

logger = logging.getLogger(__name__)

def load_resume(resume_path):
    """
    Load the resume text from a file.
    If your resume is a PDF, consider using a PDF parser (e.g., PyPDF2) to extract the text.
    """
    with open(resume_path, 'r', encoding='utf-8') as f:
        return f.read()

def preprocess_text(text):
    """
    Preprocess text by lowercasing and removing non-alphanumeric characters.
    You can expand this function to include more sophisticated preprocessing.
    """
    text = text.lower()
    # Remove punctuation and non-word characters
    text = re.sub(r'[\W_]+', ' ', text)
    return text

def build_tfidf_vectors(resume_text, job_texts):
    """
    Create TF-IDF vectors for the resume and a list of job descriptions.
    Returns the resume vector, job vectors, and the fitted vectorizer.
    """
    documents = [resume_text] + job_texts
    vectorizer = TfidfVectorizer(stop_words='english')
    tfidf_matrix = vectorizer.fit_transform(documents)
    resume_vector = tfidf_matrix[0]
    job_vectors = tfidf_matrix[1:]
    return resume_vector, job_vectors, vectorizer

def compute_similarity(resume_vector, job_vectors):
    """
    Compute cosine similarity between the resume vector and each job vector.
    Returns a list of similarity scores.
    """
    similarities = cosine_similarity(resume_vector, job_vectors)
    return similarities.flatten()

def parse_resume_and_match(resume_path, job_texts):
    """
    High-level function that loads and preprocesses the resume,
    builds TF-IDF vectors for both the resume and the job texts,
    and computes similarity scores.
    
    Parameters:
      resume_path (str): Path to the resume file.
      job_texts (List[str]): List of job description texts.
    
    Returns:
      List[float]: Similarity scores for each job description.
    """
    # Load and preprocess resume
    resume_text = load_resume(resume_path)
    resume_text = preprocess_text(resume_text)
    
    # Preprocess each job description
    processed_jobs = [preprocess_text(job) for job in job_texts]
    
    # Build TF-IDF vectors and compute similarity scores
    resume_vector, job_vectors, _ = build_tfidf_vectors(resume_text, processed_jobs)
    similarity_scores = compute_similarity(resume_vector, job_vectors)
    return similarity_scores

def update_tfidf_similarity(resume_path, db_file):
    """
    Update the TF-IDF similarity scores for job descriptions stored in the database.
    
    This function retrieves all job descriptions from the database, computes
    similarity scores against the resume, and updates the database with these scores.
    
    Parameters:
      resume_path (str): Path to the resume file.
      db_file (str): Path to the SQLite database file.
    """
    try:
        conn = sqlite3.connect(db_file)
        cur = conn.cursor()
        cur.execute("SELECT job_id, description FROM jobs")
        jobs = cur.fetchall()
        if not jobs:
            logger.info("No job entries found to update TF-IDF scores.")
            return
        job_ids = [job_id for job_id, _ in jobs]
        job_texts = [desc for _, desc in jobs]
        similarity_scores = parse_resume_and_match(resume_path, job_texts)
        logger.info("Computed TF-IDF similarity scores for %d jobs.", len(similarity_scores))
        for job_id, score in zip(job_ids, similarity_scores):
            cur.execute("UPDATE jobs SET similarity_score = ? WHERE job_id = ?", (score, job_id))
        conn.commit()
        logger.info("Updated TF-IDF similarity scores for %d jobs.", len(job_ids))
    except Exception as e:
        logger.error("Error updating TF-IDF similarity scores: %s", e)
    finally:
        if conn:
            conn.close()

# For testing purposes, you can run this module directly.
if __name__ == '__main__':
    # Assume you have a plain text resume file named 'resume.txt' in your project root.
    resume_file = os.path.join(os.getcwd(), 'resume.txt')
    # Sample job descriptions (in reality, these would come from your job scraping pipeline)
    sample_jobs = [
        "Looking for a quantitative analyst with strong mathematical background and programming skills in Python.",
        "Software engineer required with expertise in web development and JavaScript frameworks.",
        "Financial analyst needed for portfolio management, trading strategies, and investment analysis."
    ]
    scores = parse_resume_and_match(resume_file, sample_jobs)
    for idx, score in enumerate(scores, start=1):
        print(f"Job {idx} similarity score: {score:.4f}")




================================================
File: utils/__init__.py
================================================



================================================
File: utils/db_utils.py
================================================
# db_utils.py

import sqlite3
from sqlite3 import Error

def create_connection(db_file):
    """
    Create a database connection to the SQLite database specified by db_file.
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        return conn
    except Error as e:
        print("Connection error:", e)
    return conn

def execute_query(conn, query, params=None):
    """
    Execute a query (INSERT, UPDATE, DELETE) and commit changes.
    """
    try:
        cur = conn.cursor()
        if params:
            cur.execute(query, params)
        else:
            cur.execute(query)
        conn.commit()
        return cur
    except Error as e:
        print("Error executing query:", e)
        return None

def execute_read_query(conn, query, params=None):
    """
    Execute a SELECT query and return fetched results.
    """
    try:
        cur = conn.cursor()
        if params:
            cur.execute(query, params)
        else:
            cur.execute(query)
        result = cur.fetchall()
        return result
    except Error as e:
        print("Error executing read query:", e)
        return None

def initialize_database(db_file, schema_file='schema.sql'):
    """
    Initialize the database using the provided schema file.
    """
    conn = create_connection(db_file)
    if conn is not None:
        try:
            with open(schema_file, 'r') as f:
                sql_script = f.read()
            cur = conn.cursor()
            cur.executescript(sql_script)
            conn.commit()
            print("Database initialized successfully.")
        except Error as e:
            print("Error initializing database:", e)
        finally:
            conn.close()
    else:
        print("Error! Cannot create the database connection.")



================================================
File: utils/logging.py
================================================
import logging
import logging.handlers
import os
import sys

def setup_logging(log_dir='logs', log_file='app.log', level=logging.INFO):
    logger = logging.getLogger()
    # If handlers already exist, return the logger to avoid duplicate messages.
    if logger.hasHandlers():
        return logger

    # Ensure the log directory exists.
    if not os.path.exists(log_dir):
        os.makedirs(log_dir)

    log_path = os.path.join(log_dir, log_file)
    logger.setLevel(level)

    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')

    # Create a stream handler with UTF-8 encoding to avoid Unicode errors.
    stream_handler = logging.StreamHandler(sys.stdout)
    try:
        stream_handler.setStream(sys.stdout)
    except Exception:
        pass
    stream_handler.setLevel(level)
    stream_handler.setFormatter(formatter)
    logger.addHandler(stream_handler)

    # Create a rotating file handler.
    file_handler = logging.handlers.RotatingFileHandler(log_path, maxBytes=10*1024*1024, backupCount=5, encoding='utf-8')
    file_handler.setLevel(level)
    file_handler.setFormatter(formatter)
    logger.addHandler(file_handler)

    return logger



